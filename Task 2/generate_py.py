# -*- coding: utf-8 -*-
"""generate.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17n5lw-mwVzQZk74zZ_JoxVmav-TNlX5O
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

model.eval()

prompt_text = "Once upon a time"

input_ids = tokenizer.encode(prompt_text, return_tensors="pt")

top_k = 50
max_length = 50 + input_ids.shape[1]

outputs = {}

# Generate with temperature 0.7 and 1.0
for temp in [0.7, 1.0]:
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            do_sample=True,
            max_length=max_length,
            top_k=top_k,
            temperature=temp
        )
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    outputs[temp] = generated_text

with open("generated_outputs.txt", "w") as f:
    for temp, text in outputs.items():
        f.write(f"--- Temperature = {temp} ---\n{text}\n\n")

print("Generated text saved to generated_outputs.txt")